{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from datasets import Dataset\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../src\")\n",
    "import util.preprocessing_util as util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"../../data/dev\")\n",
    "processed_dir = DATA_DIR / \"processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "datasets = ['covidqa', 'cuad', 'delucionqa', 'emanual', 'expertqa', 'finqa', 'hagrid', 'hotpotqa', 'msmarco', 'pubmedqa', 'tatqa', 'techqa']\n",
    "\n",
    "# load pubmedqa from RAGBENCH\n",
    "pubmedqa = load_dataset(\"rungalileo/ragbench\", \"pubmedqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = pubmedqa['train']\n",
    "test_data = pubmedqa['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = train_data.to_pandas()\n",
    "test_data = test_data.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>documents</th>\n",
       "      <th>response</th>\n",
       "      <th>generation_model_name</th>\n",
       "      <th>annotating_model_name</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>documents_sentences</th>\n",
       "      <th>response_sentences</th>\n",
       "      <th>sentence_support_information</th>\n",
       "      <th>...</th>\n",
       "      <th>trulens_groundedness</th>\n",
       "      <th>trulens_context_relevance</th>\n",
       "      <th>ragas_faithfulness</th>\n",
       "      <th>ragas_context_relevance</th>\n",
       "      <th>gpt3_adherence</th>\n",
       "      <th>gpt3_context_relevance</th>\n",
       "      <th>gpt35_utilization</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>utilization_score</th>\n",
       "      <th>completeness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pubmedqa_39085</td>\n",
       "      <td>Is there a functional neural correlate of indi...</td>\n",
       "      <td>[The present study tested whether individuals ...</td>\n",
       "      <td>Yes, the study mentioned in the first piece of...</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>pubmedqa_train</td>\n",
       "      <td>[[[0a, The present study tested whether indivi...</td>\n",
       "      <td>[[a, Yes, the study mentioned in the first pie...</td>\n",
       "      <td>[{'explanation': 'This sentence is directly su...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pubmedqa_42813</td>\n",
       "      <td>Can we use the Omron T9P automated blood press...</td>\n",
       "      <td>[Recent events in our hospital, combined with ...</td>\n",
       "      <td>Yes, based on the context provided, we can use...</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>pubmedqa_train</td>\n",
       "      <td>[[[0a, Recent events in our hospital, combined...</td>\n",
       "      <td>[[a, Yes, based on the context provided, we ca...</td>\n",
       "      <td>[{'explanation': 'This sentence summarizes the...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pubmedqa_57525</td>\n",
       "      <td>Intraabdominal vascular injury: are we getting...</td>\n",
       "      <td>[Intraabdominal vascular injury (IAVI) as a re...</td>\n",
       "      <td>Based on the pieces of context provided, the s...</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>pubmedqa_train</td>\n",
       "      <td>[[[0a, Intraabdominal vascular injury (IAVI) a...</td>\n",
       "      <td>[[a, Based on the pieces of context provided, ...</td>\n",
       "      <td>[{'explanation': 'This sentence is partially s...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pubmedqa_29863</td>\n",
       "      <td>Hand grip and pinch strength in patients with ...</td>\n",
       "      <td>[The hand grip strength test and pinch was sig...</td>\n",
       "      <td>Yes, according to the context provided, the ha...</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>pubmedqa_train</td>\n",
       "      <td>[[[0a, The hand grip strength test and pinch w...</td>\n",
       "      <td>[[a, Yes, according to the context provided, t...</td>\n",
       "      <td>[{'explanation': 'This sentence is supported b...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pubmedqa_36126</td>\n",
       "      <td>Is Canada ready for patient accessible electro...</td>\n",
       "      <td>[Access to personal health information through...</td>\n",
       "      <td>Based on the information provided, Canada is n...</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>pubmedqa_train</td>\n",
       "      <td>[[[0a, Access to personal health information t...</td>\n",
       "      <td>[[a, Based on the information provided, Canada...</td>\n",
       "      <td>[{'explanation': 'The sentence is supported by...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                           question  \\\n",
       "0  pubmedqa_39085  Is there a functional neural correlate of indi...   \n",
       "1  pubmedqa_42813  Can we use the Omron T9P automated blood press...   \n",
       "2  pubmedqa_57525  Intraabdominal vascular injury: are we getting...   \n",
       "3  pubmedqa_29863  Hand grip and pinch strength in patients with ...   \n",
       "4  pubmedqa_36126  Is Canada ready for patient accessible electro...   \n",
       "\n",
       "                                           documents  \\\n",
       "0  [The present study tested whether individuals ...   \n",
       "1  [Recent events in our hospital, combined with ...   \n",
       "2  [Intraabdominal vascular injury (IAVI) as a re...   \n",
       "3  [The hand grip strength test and pinch was sig...   \n",
       "4  [Access to personal health information through...   \n",
       "\n",
       "                                            response generation_model_name  \\\n",
       "0  Yes, the study mentioned in the first piece of...    gpt-3.5-turbo-0125   \n",
       "1  Yes, based on the context provided, we can use...    gpt-3.5-turbo-0125   \n",
       "2  Based on the pieces of context provided, the s...    gpt-3.5-turbo-0125   \n",
       "3  Yes, according to the context provided, the ha...    gpt-3.5-turbo-0125   \n",
       "4  Based on the information provided, Canada is n...    gpt-3.5-turbo-0125   \n",
       "\n",
       "  annotating_model_name    dataset_name  \\\n",
       "0                gpt-4o  pubmedqa_train   \n",
       "1                gpt-4o  pubmedqa_train   \n",
       "2                gpt-4o  pubmedqa_train   \n",
       "3                gpt-4o  pubmedqa_train   \n",
       "4                gpt-4o  pubmedqa_train   \n",
       "\n",
       "                                 documents_sentences  \\\n",
       "0  [[[0a, The present study tested whether indivi...   \n",
       "1  [[[0a, Recent events in our hospital, combined...   \n",
       "2  [[[0a, Intraabdominal vascular injury (IAVI) a...   \n",
       "3  [[[0a, The hand grip strength test and pinch w...   \n",
       "4  [[[0a, Access to personal health information t...   \n",
       "\n",
       "                                  response_sentences  \\\n",
       "0  [[a, Yes, the study mentioned in the first pie...   \n",
       "1  [[a, Yes, based on the context provided, we ca...   \n",
       "2  [[a, Based on the pieces of context provided, ...   \n",
       "3  [[a, Yes, according to the context provided, t...   \n",
       "4  [[a, Based on the information provided, Canada...   \n",
       "\n",
       "                        sentence_support_information  ...  \\\n",
       "0  [{'explanation': 'This sentence is directly su...  ...   \n",
       "1  [{'explanation': 'This sentence summarizes the...  ...   \n",
       "2  [{'explanation': 'This sentence is partially s...  ...   \n",
       "3  [{'explanation': 'This sentence is supported b...  ...   \n",
       "4  [{'explanation': 'The sentence is supported by...  ...   \n",
       "\n",
       "  trulens_groundedness  trulens_context_relevance ragas_faithfulness  \\\n",
       "0                  NaN                        NaN                NaN   \n",
       "1                  NaN                        NaN                NaN   \n",
       "2                  NaN                        NaN                NaN   \n",
       "3                  NaN                        NaN                NaN   \n",
       "4                  NaN                        NaN                NaN   \n",
       "\n",
       "  ragas_context_relevance gpt3_adherence gpt3_context_relevance  \\\n",
       "0                     NaN            NaN                    NaN   \n",
       "1                     NaN            NaN                    NaN   \n",
       "2                     NaN            NaN                    NaN   \n",
       "3                     NaN            NaN                    NaN   \n",
       "4                     NaN            NaN                    NaN   \n",
       "\n",
       "   gpt35_utilization  relevance_score  utilization_score  completeness_score  \n",
       "0                NaN         0.142857           0.142857            1.000000  \n",
       "1                NaN         0.454545           0.181818            0.400000  \n",
       "2                NaN         0.461538           0.307692            0.666667  \n",
       "3                NaN         0.818182           0.181818            0.222222  \n",
       "4                NaN         0.636364           0.454545            0.714286  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19600, 26)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'question', 'documents', 'response', 'generation_model_name',\n",
       "       'annotating_model_name', 'dataset_name', 'documents_sentences',\n",
       "       'response_sentences', 'sentence_support_information',\n",
       "       'unsupported_response_sentence_keys', 'adherence_score',\n",
       "       'overall_supported_explanation', 'relevance_explanation',\n",
       "       'all_relevant_sentence_keys', 'all_utilized_sentence_keys',\n",
       "       'trulens_groundedness', 'trulens_context_relevance',\n",
       "       'ragas_faithfulness', 'ragas_context_relevance', 'gpt3_adherence',\n",
       "       'gpt3_context_relevance', 'gpt35_utilization', 'relevance_score',\n",
       "       'utilization_score', 'completeness_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Series.unique of 0                                        [0a]\n",
       "1                        [0a, 1a, 3a, 3b, 4a]\n",
       "2                    [0a, 0b, 2a, 3c, 3e, 4a]\n",
       "3        [0a, 0b, 1a, 2a, 2b, 2c, 2d, 2e, 2f]\n",
       "4                [0a, 0b, 0c, 0d, 1a, 1b, 1c]\n",
       "                         ...                 \n",
       "19595        [0a, 0b, 1a, 1b, 2a, 2c, 2d, 3a]\n",
       "19596                    [0a, 0b, 0c, 0d, 1b]\n",
       "19597                [0a, 1a, 2a, 3a, 3d, 4c]\n",
       "19598        [0b, 0c, 0d, 0e, 1a, 1b, 1c, 1d]\n",
       "19599                [0b, 1a, 1b, 1c, 4a, 4f]\n",
       "Name: all_relevant_sentence_keys, Length: 19600, dtype: object>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.all_relevant_sentence_keys.unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "observation = train_data.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Can we use the Omron T9P automated blood pressure monitor in pregnancy?'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation.question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0a', '1a', '3a', '3b', '4a'], dtype=object)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation.all_relevant_sentence_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**question:** The medical question being asked. (Used as input)\n",
    "**documents_sentences:** Contains the sentences from the source documents. (Context for classification)\n",
    "**all_relevant_sentence_keys:**\tIdentifies which sentences are relevant. (Binary label for classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**all_relevant_sentence_keys:**\n",
    "\n",
    "- This column contains sentence identifiers (e.g., '0a', '1b') that point to relevant sentences in documents_sentences.\n",
    "- Format: 'Xd', where X is the index of the retrieved document, and d is the sentence ID within that document.\n",
    "- These keys identify which sentences from the retrieved documents were deemed relevant for answering the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:**\n",
    "\n",
    "- Pair each question with its individual document sentences → (question, sentence) pairs.\n",
    "- Label each sentence as relevant (1) or not relevant (0) → Using all_relevant_sentence_keys.\n",
    "- Train a classifier using BERT embeddings to classify each sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = train_data[[\"question\", \"documents_sentences\", \"all_relevant_sentence_keys\"]]\n",
    "test_data = test_data[[\"question\", \"documents_sentences\", \"all_relevant_sentence_keys\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribute Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def safe_eval(val):\n",
    "    if isinstance(val, str):  # If it's a string, evaluate it\n",
    "        return ast.literal_eval(val)\n",
    "    elif isinstance(val, list) or isinstance(val, tuple):  # If already a list, return as is\n",
    "        return list(val)\n",
    "    else:\n",
    "        return []  # Default case, return empty list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data.loc[:, \"documents_sentences\"] = train_data[\"documents_sentences\"] \\\n",
    "    .apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "test_data.loc[:, \"documents_sentences\"] = test_data[\"documents_sentences\"] \\\n",
    "    .apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_dataset(train_data):\n",
    "    \"\"\"\n",
    "    Transforms the dataset into a list of (question, sentence, label) tuples.\n",
    "\n",
    "    Args:\n",
    "        train_data (pd.DataFrame): DataFrame containing 'question', 'all_relevant_sentence_keys', and 'documents_sentences'.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples (question, sentence_text, label).\n",
    "    \"\"\"\n",
    "    data_rows = []\n",
    "\n",
    "    for _, row in train_data.iterrows():\n",
    "        question = row[\"question\"]\n",
    "        relevant_keys = set(row[\"all_relevant_sentence_keys\"])  # Convert to set for quick lookup\n",
    "\n",
    "        for doc_sentences in row[\"documents_sentences\"]:\n",
    "            for sentence in doc_sentences:  # sentence is a list like [key, text]\n",
    "                if len(sentence) == 2:  # Ensure correct format\n",
    "                    sentence_key, sentence_text = sentence\n",
    "                    label = 1 if sentence_key in relevant_keys else 0  # Assign label\n",
    "                    data_rows.append((question, sentence_text, label))\n",
    "\n",
    "    return data_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame(transform_dataset(train_data), columns=[\"question\", \"sentence\", \"label\"])\n",
    "test_data = pd.DataFrame(transform_dataset(test_data), columns=[\"question\", \"sentence\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is there a functional neural correlate of indi...</td>\n",
       "      <td>The present study tested whether individuals w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Is there a functional neural correlate of indi...</td>\n",
       "      <td>This study examined whether heightened cardiov...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is there a functional neural correlate of indi...</td>\n",
       "      <td>Heart rate variability (HRV), a measure of aut...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is there a functional neural correlate of indi...</td>\n",
       "      <td>Previous studies have also not controlled for ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Is there a functional neural correlate of indi...</td>\n",
       "      <td>Low socioeconomic status is associated with in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Is there a functional neural correlate of indi...   \n",
       "1  Is there a functional neural correlate of indi...   \n",
       "2  Is there a functional neural correlate of indi...   \n",
       "3  Is there a functional neural correlate of indi...   \n",
       "4  Is there a functional neural correlate of indi...   \n",
       "\n",
       "                                            sentence  label  \n",
       "0  The present study tested whether individuals w...      1  \n",
       "1  This study examined whether heightened cardiov...      0  \n",
       "2  Heart rate variability (HRV), a measure of aut...      0  \n",
       "3  Previous studies have also not controlled for ...      0  \n",
       "4  Low socioeconomic status is associated with in...      0  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question    object\n",
       "sentence    object\n",
       "label        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_data[\"question\"] = train_data[\"question\"].astype(str)\n",
    "train_data[\"sentence\"] = train_data[\"sentence\"].astype(str)\n",
    "test_data[\"question\"] = test_data[\"question\"].astype(str)\n",
    "test_data[\"sentence\"] = test_data[\"sentence\"].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question    0\n",
      "sentence    0\n",
      "label       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We might want to check for duplicates here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert strings to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_data[\"question\"] = train_data[\"question\"].str.lower()\n",
    "train_data[\"sentence\"] = train_data[\"sentence\"].str.lower()\n",
    "test_data[\"question\"] = test_data[\"question\"].str.lower()\n",
    "test_data[\"sentence\"] = test_data[\"sentence\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Extra Whitespaces & Newlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def clean_whitespace(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces/newlines with a single space\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_data[\"question\"] = train_data[\"question\"].apply(clean_whitespace)\n",
    "train_data[\"sentence\"] = train_data[\"sentence\"].apply(clean_whitespace)\n",
    "test_data[\"question\"] = test_data[\"question\"].apply(clean_whitespace)\n",
    "test_data[\"sentence\"] = test_data[\"sentence\"].apply(clean_whitespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize Unicode Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def normalize_unicode(text):\n",
    "    return unicodedata.normalize(\"NFKC\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_data[\"question\"] = train_data[\"question\"].apply(normalize_unicode)\n",
    "train_data[\"sentence\"] = train_data[\"sentence\"].apply(normalize_unicode)\n",
    "test_data[\"question\"] = test_data[\"question\"].apply(normalize_unicode)\n",
    "test_data[\"sentence\"] = test_data[\"sentence\"].apply(normalize_unicode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_data[\"question\"] = train_data[\"question\"].apply(remove_punctuation)\n",
    "train_data[\"sentence\"] = train_data[\"sentence\"].apply(remove_punctuation)\n",
    "test_data[\"question\"] = test_data[\"question\"].apply(remove_punctuation)\n",
    "test_data[\"sentence\"] = test_data[\"sentence\"].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Safe if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "train_data.to_csv(\"../../data/dev/processed/pubmedqa_train.csv\", index=False)\n",
    "test_data.to_csv(\"../../data/dev/processed/pubmedqa_test.csv\", index=False)\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is there a functional neural correlate of indi...</td>\n",
       "      <td>the present study tested whether individuals w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is there a functional neural correlate of indi...</td>\n",
       "      <td>this study examined whether heightened cardiov...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is there a functional neural correlate of indi...</td>\n",
       "      <td>heart rate variability hrv a measure of autono...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is there a functional neural correlate of indi...</td>\n",
       "      <td>previous studies have also not controlled for ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is there a functional neural correlate of indi...</td>\n",
       "      <td>low socioeconomic status is associated with in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  is there a functional neural correlate of indi...   \n",
       "1  is there a functional neural correlate of indi...   \n",
       "2  is there a functional neural correlate of indi...   \n",
       "3  is there a functional neural correlate of indi...   \n",
       "4  is there a functional neural correlate of indi...   \n",
       "\n",
       "                                            sentence  label  \n",
       "0  the present study tested whether individuals w...      1  \n",
       "1  this study examined whether heightened cardiov...      0  \n",
       "2  heart rate variability hrv a measure of autono...      0  \n",
       "3  previous studies have also not controlled for ...      0  \n",
       "4  low socioeconomic status is associated with in...      0  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mask Sentences**\n",
    "\n",
    "- Replace all sentences except one with a [MASK] token\n",
    "- Keep only one sentence unmasked at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def mask_sentences(sentences, target_idx):\n",
    "    \"\"\"\n",
    "    Masks all sentences except the one at target_idx.\n",
    "    \"\"\"\n",
    "    return \" \".join(\n",
    "        [sent if idx == target_idx else \"[MASK]\" for idx, (sid, sent) in enumerate(sentences)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate Sentences Per Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Dictionary to store aggregated data\n",
    "aggregated_train_data = defaultdict(lambda: {\"sentences\": [], \"labels\": []})\n",
    "aggregated_test_data = defaultdict(lambda: {\"sentences\": [], \"labels\": []})\n",
    "\n",
    "# aggregate sentences per question\n",
    "for _, row in train_data.iterrows():\n",
    "    q = row[\"question\"]\n",
    "    aggregated_train_data[q][\"sentences\"].append(row[\"sentence\"])\n",
    "    aggregated_train_data[q][\"labels\"].append(row[\"label\"])\n",
    "\n",
    "for _, row in test_data.iterrows():\n",
    "    q = row[\"question\"]\n",
    "    aggregated_test_data[q][\"sentences\"].append(row[\"sentence\"])\n",
    "    aggregated_test_data[q][\"labels\"].append(row[\"label\"])\n",
    "\n",
    "# convert to df\n",
    "aggregated_train_df = pd.DataFrame([\n",
    "    {\"question\": q, \"sentences\": v[\"sentences\"], \"labels\": v[\"labels\"]}\n",
    "    for q, v in aggregated_train_data.items()\n",
    "])\n",
    "aggregated_test_df = pd.DataFrame([\n",
    "    {\"question\": q, \"sentences\": v[\"sentences\"], \"labels\": v[\"labels\"]}\n",
    "    for q, v in aggregated_test_data.items()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mask Sentences One by One - Create multiple masked versions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def create_masked_inputs(df):\n",
    "    masked_data = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        question = row[\"question\"]\n",
    "        sentences = row[\"sentences\"]\n",
    "        labels = row[\"labels\"]\n",
    "\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            # Mask all sentences except the one at index i\n",
    "            masked_context = [\"[MASK]\" if j != i else s for j, s in enumerate(sentences)]\n",
    "            full_context = \" \".join(masked_context)\n",
    "\n",
    "            # Store new training instance\n",
    "            masked_data.append({\"question\": question, \"context\": full_context, \"label\": labels[i]})\n",
    "\n",
    "    return pd.DataFrame(masked_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "masked_train_df = create_masked_inputs(aggregated_train_df)\n",
    "masked_test_df = create_masked_inputs(aggregated_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is there a functional neural correlate of indi...</td>\n",
       "      <td>the present study tested whether individuals w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is there a functional neural correlate of indi...</td>\n",
       "      <td>[MASK] this study examined whether heightened ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is there a functional neural correlate of indi...</td>\n",
       "      <td>[MASK] [MASK] heart rate variability hrv a mea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is there a functional neural correlate of indi...</td>\n",
       "      <td>[MASK] [MASK] [MASK] previous studies have als...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is there a functional neural correlate of indi...</td>\n",
       "      <td>[MASK] [MASK] [MASK] [MASK] low socioeconomic ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  is there a functional neural correlate of indi...   \n",
       "1  is there a functional neural correlate of indi...   \n",
       "2  is there a functional neural correlate of indi...   \n",
       "3  is there a functional neural correlate of indi...   \n",
       "4  is there a functional neural correlate of indi...   \n",
       "\n",
       "                                             context  label  \n",
       "0  the present study tested whether individuals w...      1  \n",
       "1  [MASK] this study examined whether heightened ...      0  \n",
       "2  [MASK] [MASK] heart rate variability hrv a mea...      0  \n",
       "3  [MASK] [MASK] [MASK] previous studies have als...      0  \n",
       "4  [MASK] [MASK] [MASK] [MASK] low socioeconomic ...      0  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization (Not End-To-End)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encode the question together with the entire context (all sentences in the document). This allows the model to have full context while classifying.\n",
    "\n",
    "However, we only predict the relevance of one sentence at a time.\n",
    "\n",
    "To achieve this, we mask the other sentences in the context so that the model focuses only on the target sentence.\n",
    "\n",
    "This ensures that during training, only the sentence being classified is \"visible\" for learning, while the rest are ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get tokenizer from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "token = \"XXXX\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\", num_labels=2, token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Normally we can create the encodings like this but since its taking very long on my laptop we use the custom version with the tqdm progress bar which is also much faster and memory efficient.\n",
    "'''\n",
    "'''\n",
    "encodings = tokenizer(\n",
    "    list(masked_df[\"question\"]),\n",
    "    list(masked_df[\"context\"]),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Convert the dataframe into lists for iteration\n",
    "questions = list(masked_train_df[\"question\"])\n",
    "contexts = list(masked_train_df[\"context\"])\n",
    "\n",
    "# Initialize an empty list to store tokenized outputs\n",
    "input_ids, attention_masks = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c53dbc0c21364f5ab90003a35bdc7a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/237544 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for question, context in tqdm(zip(questions, contexts), total=len(questions), desc=\"Tokenizing\"):\n",
    "    encoding = tokenizer(\n",
    "        question,\n",
    "        context,\n",
    "        padding=\"max_length\",  # Ensures uniform length\n",
    "        truncation=True,       # Cuts off if too long\n",
    "        max_length=512,        # ModernBERT supports up to 512 tokens\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    input_ids.append(encoding[\"input_ids\"].squeeze())  # Remove batch dim\n",
    "    attention_masks.append(encoding[\"attention_mask\"].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dataset shape: torch.Size([237544, 512])\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.stack(input_ids)\n",
    "attention_masks = torch.stack(attention_masks)\n",
    "\n",
    "# Check the shape of tokenized data\n",
    "print(f\"Tokenized dataset shape: {input_ids.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Encodings to PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 237544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m4/prj94mxs4pn6lk9nc4z7vrc80000gn/T/ipykernel_24743/517551896.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.labels = torch.tensor(labels)  # Convert labels to tensor\n"
     ]
    }
   ],
   "source": [
    "class SentenceRelevanceDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.labels = torch.tensor(labels)  # Convert labels to tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_masks[idx],\n",
    "            \"labels\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "# Convert labels to tensor\n",
    "labels = torch.tensor(list(masked_train_df[\"label\"]))\n",
    "\n",
    "# Create dataset\n",
    "dataset = SentenceRelevanceDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 190035\n",
      "Validation samples: 47509\n"
     ]
    }
   ],
   "source": [
    "# Define split ratio (80% train, 20% validation)\n",
    "train_size = 0.8\n",
    "train_idx, val_idx = train_test_split(\n",
    "    range(len(dataset)), test_size=1 - train_size, random_state=1050\n",
    ")\n",
    "\n",
    "# Create train and validation subsets\n",
    "from torch.utils.data import Subset\n",
    "train_dataset = Subset(dataset, train_idx)\n",
    "val_dataset = Subset(dataset, val_idx)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training End-To-End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load ModernBERT Model (Trainable Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"answerdotai/ModernBERT-base\", num_labels=2, token=token\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if model parameters are trainable (Ensure We Are Training End-to-End)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True  # Ensure the entire model is updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "hf_dataset_train = Dataset.from_pandas(masked_train_df)\n",
    "hf_dataset_test = Dataset.from_pandas(masked_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "781e60548aa44f779ee924cc5e0ada60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/237544 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a global tqdm progress bar\n",
    "progress_bar = tqdm(total=(len(hf_dataset_train) + len(hf_dataset_test)),\n",
    "                    desc=\"Tokenizing\", position=0, leave=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_batch(batch):\n",
    "    \"\"\"Tokenizes question + masked context while updating tqdm.\"\"\"\n",
    "\n",
    "    # Tokenize batch\n",
    "    encodings = tokenizer(\n",
    "        batch[\"question\"],\n",
    "        batch[\"context\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Update tqdm manually (batch_size at a time)\n",
    "    progress_bar.update(len(batch[\"question\"]))\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": encodings[\"input_ids\"].tolist(),\n",
    "        \"attention_mask\": encodings[\"attention_mask\"].tolist(),\n",
    "        \"labels\": batch[\"label\"]  # Keep labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize_batch at 0x2b2534d30> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a937bbc62184351b00dd75465bf10f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/237544 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset_train = hf_dataset_train.map(\n",
    "    tokenize_batch,\n",
    "    batched=True,\n",
    "    batch_size=32  # Adjust based on your system\n",
    ")\n",
    "tokenized_dataset_test = hf_dataset_test.map(\n",
    "    tokenize_batch,\n",
    "    batched=True,\n",
    "    batch_size=32  # Adjust based on your system\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 190035, Test samples: 47509\n"
     ]
    }
   ],
   "source": [
    "# Ensure dataset is set to PyTorch format\n",
    "tokenized_dataset_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "tokenized_dataset_test.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Split dataset into train/test\n",
    "'''\n",
    "split = 0.8\n",
    "train_size = int(split * len(tokenized_dataset))\n",
    "test_size = len(tokenized_dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(tokenized_dataset, [train_size, test_size])\n",
    "'''\n",
    "\n",
    "print(f\"Training samples: {len(tokenized_dataset_train)}, Test samples: {len(tokenized_dataset_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create DataLoader for Batching (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Check one batch\n",
    "batch = next(iter(train_dataloader))\n",
    "print({key: value.shape for key, value in batch.items()})\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick check if everything is correct before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'context', 'label', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 237544\n",
      "})\n",
      "['question', 'context', 'label', 'input_ids', 'attention_mask', 'labels']\n"
     ]
    }
   ],
   "source": [
    "print(\"----- Training Set -----\")\n",
    "# Check correct format\n",
    "print(tokenized_dataset_train)\n",
    "# Check required fields\n",
    "print(tokenized_dataset_train.column_names)\n",
    "print(\"----- Test Set -----\")\n",
    "# Check correct format\n",
    "print(tokenized_dataset_test)\n",
    "# Check required fields\n",
    "print(tokenized_dataset_test.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "output_dir = Path(\"../../results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset_train,\n",
    "    eval_dataset=tokenized_dataset_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
