{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DocumentProcessor Testing Notebook\n",
    "\n",
    "This notebook tests the DocumentProcessor functionality with different chunking strategies and document types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup and Imports\n\nThis section sets up the Python environment, handles common ML library conflicts (OpenMP), and imports the necessary modules for testing the DocumentProcessor."
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/paulschmitt/DataspellProjects/verbatim-rag\n",
      "Current working directory: /Users/paulschmitt/DataspellProjects/verbatim-rag/notebooks\n",
      "‚úÖ OpenMP conflict workaround applied\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Fix OpenMP conflict (common with ML libraries on macOS)\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "# Add the project root to Python path\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Current working directory: {Path.cwd()}\")\n",
    "print(\"‚úÖ OpenMP conflict workaround applied\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from verbatim_rag.ingestion import DocumentProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from verbatim_rag.document import DocumentType, ChunkType\n",
    "import json\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Check Available Test Files\n\nBefore running tests, we need to verify which document files are available for processing. This helps us understand what test data we have to work with."
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example docs path: /Users/paulschmitt/DataspellProjects/verbatim-rag/data/acl_papers\n",
      "Exists: True\n",
      "\n",
      "Available files:\n",
      "  - VERBATIM_RAG_ACL.pdf (362982 bytes)\n"
     ]
    }
   ],
   "source": [
    "# Check available example documents\n",
    "example_docs_path = project_root / \"data\" / \"acl_papers\"\n",
    "print(f\"Example docs path: {example_docs_path}\")\n",
    "print(f\"Exists: {example_docs_path.exists()}\")\n",
    "\n",
    "if example_docs_path.exists():\n",
    "    print(\"\\nAvailable files:\")\n",
    "    for file in example_docs_path.iterdir():\n",
    "        print(f\"  - {file.name} ({file.stat().st_size} bytes)\")\n",
    "else:\n",
    "    print(\"Example docs directory not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Test 1: Basic DocumentProcessor Creation\n\nThis test verifies that the DocumentProcessor can be instantiated with default settings. It checks if all required dependencies (docling, chonkie) are properly installed and accessible."
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DocumentProcessor created successfully\n",
      "Chunker type: recursive\n",
      "Chunk size: 512\n",
      "Chunk overlap: 50\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Test basic creation with default settings\n",
    "    processor = DocumentProcessor()\n",
    "    print(\"‚úÖ DocumentProcessor created successfully\")\n",
    "    print(f\"Chunker type: {processor.chunker_type}\")\n",
    "    print(f\"Chunk size: {processor.chunk_size}\")\n",
    "    print(f\"Chunk overlap: {processor.chunk_overlap}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating DocumentProcessor: {e}\")\n",
    "    print(\"Make sure you have installed the document-processing dependencies:\")\n",
    "    print(\"pip install -e .[document-processing]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Test 2: Process a Simple Text File\n\nThis test processes a sample markdown document to understand how the DocumentProcessor converts content into chunks. We create a test document with headers and sections to analyze the chunking behavior."
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_name = \"VERBATIM_RAG_ACL.pdf\"\n",
    "test_file_path = example_docs_path / test_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulschmitt/miniforge3/envs/verbatim-rag-2/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Document processed successfully!\n",
      "Document ID: 7c2a73ad-6491-45ab-a371-8717b4d72a18\n",
      "Title: VERBATIM_RAG_ACL.pdf\n",
      "Source: /Users/paulschmitt/DataspellProjects/verbatim-rag/data/acl_papers/VERBATIM_RAG_ACL.pdf\n",
      "Content Type: DocumentType.PDF\n",
      "Number of chunks: 88\n",
      "Raw content length: 25746 characters\n"
     ]
    }
   ],
   "source": [
    "# Test processing the file\n",
    "try:\n",
    "    processor = DocumentProcessor()\n",
    "    document = processor.process_file(test_file_path, title=test_file_name)\n",
    "    \n",
    "    print(\"‚úÖ Document processed successfully!\")\n",
    "    print(f\"Document ID: {document.id}\")\n",
    "    print(f\"Title: {document.title}\")\n",
    "    print(f\"Source: {document.source}\")\n",
    "    print(f\"Content Type: {document.content_type}\")\n",
    "    print(f\"Number of chunks: {len(document.chunks)}\")\n",
    "    print(f\"Raw content length: {len(document.raw_content)} characters\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error processing document: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Test 3: Examine Chunks in Detail\n\nThis test provides a detailed analysis of the generated chunks, including their structure, content, and metadata. It helps us understand how content is split and what information is preserved in each chunk."
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Document Analysis:\n",
      "Total chunks: 88\n",
      "\n",
      "üìù Chunk Details:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "ID: 9ca6dcc7-f0e1-438b-96d7-a0e227425290\n",
      "Type: ChunkType.PARAGRAPH\n",
      "Number: 0\n",
      "Content length: 227 chars\n",
      "Processed chunks: 1\n",
      "Content preview: ## KR Labs at ArchEHR-QA 2025: A Verbatim Approach for Evidence-Based Question Answering\n",
      "\n",
      "√Åd√°m Kov√°cs KR Labs kovacs@krlabs.eu\n",
      "\n",
      "## Paul Schmitt\n",
      "\n",
      "TU Wien paul.schmitt@tuwien.ac.at\n",
      "\n",
      "G√°bor Recski KR Labs...\n",
      "Enhanced content length: 227 chars\n",
      "Enhanced preview: ## KR Labs at ArchEHR-QA 2025: A Verbatim Approach for Evidence-Based Question Answering\n",
      "\n",
      "√Åd√°m Kov√°cs KR Labs kovacs@krlabs.eu\n",
      "\n",
      "## Paul Schmitt\n",
      "\n",
      "TU Wien paul.schmitt@tuwien.ac.at\n",
      "\n",
      "G√°bor Recski KR Labs...\n",
      "\n",
      "--- Chunk 2 ---\n",
      "ID: 408e5810-c9ac-4d0b-9faa-9504d8008d27\n",
      "Type: ChunkType.PARAGRAPH\n",
      "Number: 1\n",
      "Content length: 458 chars\n",
      "Processed chunks: 1\n",
      "Content preview: ## Abstract\n",
      "\n",
      "We present a lightweight, domain-agnostic verbatim pipeline for evidence-grounded question answering. Our pipeline operates in two steps: first, a sentence-level extractor flags relevant ...\n",
      "Enhanced content length: 458 chars\n",
      "Enhanced preview: ## Abstract\n",
      "\n",
      "We present a lightweight, domain-agnostic verbatim pipeline for evidence-grounded question answering. Our pipeline operates in two steps: first, a sentence-level extractor flags relevant ...\n",
      "\n",
      "--- Chunk 3 ---\n",
      "ID: f8f4fdd6-282c-476d-b8d1-3539dababd2f\n",
      "Type: ChunkType.PARAGRAPH\n",
      "Number: 2\n",
      "Content length: 234 chars\n",
      "Processed chunks: 1\n",
      "Content preview: In the ArchEHR-QA 2025 shared task, our system scored 42.01%, ranking top-10 in core metrics and outperforming the organiser's 70B-parameter Llama-3.3 baseline. We publicly release our code and infere...\n",
      "Enhanced content length: 234 chars\n",
      "Enhanced preview: In the ArchEHR-QA 2025 shared task, our system scored 42.01%, ranking top-10 in core metrics and outperforming the organiser's 70B-parameter Llama-3.3 baseline. We publicly release our code and infere...\n",
      "\n",
      "--- Chunk 4 ---\n",
      "ID: 6e240427-434c-4c97-83fd-a2bd9229ac88\n",
      "Type: ChunkType.PARAGRAPH\n",
      "Number: 3\n",
      "Content length: 1 chars\n",
      "Processed chunks: 1\n",
      "Content preview: \n",
      "...\n",
      "Enhanced content length: 1 chars\n",
      "Enhanced preview: \n",
      "...\n",
      "\n",
      "--- Chunk 5 ---\n",
      "ID: 03a812fc-ba4d-4828-bb43-e191ceb9f9e9\n",
      "Type: ChunkType.PARAGRAPH\n",
      "Number: 4\n",
      "Content length: 68 chars\n",
      "Processed chunks: 1\n",
      "Content preview: sively with verbatim sentences selected from the extraction phase.\n",
      "\n",
      "...\n",
      "Enhanced content length: 68 chars\n",
      "Enhanced preview: sively with verbatim sentences selected from the extraction phase.\n",
      "\n",
      "...\n",
      "\n",
      "... and 83 more chunks\n"
     ]
    }
   ],
   "source": [
    "if 'document' in locals():\n",
    "    print(f\"\\nüìÑ Document Analysis:\")\n",
    "    print(f\"Total chunks: {len(document.chunks)}\")\n",
    "    \n",
    "    print(\"\\nüìù Chunk Details:\")\n",
    "    for i, chunk in enumerate(document.chunks[:5]):  # Show first 5 chunks\n",
    "        print(f\"\\n--- Chunk {i+1} ---\")\n",
    "        print(f\"ID: {chunk.id}\")\n",
    "        print(f\"Type: {chunk.chunk_type}\")\n",
    "        print(f\"Number: {chunk.chunk_number}\")\n",
    "        print(f\"Content length: {len(chunk.content)} chars\")\n",
    "        print(f\"Processed chunks: {len(chunk.processed_chunks)}\")\n",
    "        print(f\"Content preview: {chunk.content[:200]}...\")\n",
    "        \n",
    "        # Show processed chunk details\n",
    "        if chunk.processed_chunks:\n",
    "            pc = chunk.processed_chunks[0]\n",
    "            print(f\"Enhanced content length: {len(pc.enhanced_content)} chars\")\n",
    "            print(f\"Enhanced preview: {pc.enhanced_content[:200]}...\")\n",
    "    \n",
    "    if len(document.chunks) > 5:\n",
    "        print(f\"\\n... and {len(document.chunks) - 5} more chunks\")\n",
    "else:\n",
    "    print(\"‚ùå No document available to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Test 4: Different Chunking Strategies\n\nThis test compares various chunking approaches (recursive, token, sentence, word) to understand how each strategy affects the resulting chunks. This comparison helps identify the best approach for different use cases."
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Testing recursive chunking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulschmitt/miniforge3/envs/verbatim-rag-2/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ recursive: 88 chunks\n",
      "\n",
      "üîÑ Testing token chunking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulschmitt/miniforge3/envs/verbatim-rag-2/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ token: 125 chunks\n",
      "\n",
      "üîÑ Testing sentence chunking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulschmitt/miniforge3/envs/verbatim-rag-2/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ sentence: 273 chunks\n",
      "\n",
      "üîÑ Testing word chunking...\n",
      "  ‚ùå word failed: module 'chonkie' has no attribute 'WordChunker'\n",
      "\n",
      "üìä Chunking Strategy Comparison:\n",
      "------------------------------------------------------------\n",
      "recursive    |  88 chunks | Avg size:  292.6 chars\n",
      "token        | 125 chunks | Avg size:  255.6 chars\n",
      "sentence     | 273 chunks | Avg size:   94.3 chars\n",
      "word         | ERROR: module 'chonkie' has no attribute 'WordChunker'\n"
     ]
    }
   ],
   "source": [
    "# Test different chunking strategies\n",
    "chunking_strategies = [\n",
    "    (\"recursive\", {\"chunker_recipe\": \"markdown\", \"chunk_size\": 512}),\n",
    "    (\"token\", {\"chunk_size\": 256, \"chunk_overlap\": 50}),\n",
    "    (\"sentence\", {\"chunk_size\": 3, \"chunk_overlap\": 1}),\n",
    "    (\"word\", {\"chunk_size\": 100, \"chunk_overlap\": 20}),\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for strategy_name, kwargs in chunking_strategies:\n",
    "    try:\n",
    "        print(f\"\\nüîÑ Testing {strategy_name} chunking...\")\n",
    "        processor = DocumentProcessor(chunker_type=strategy_name, **kwargs)\n",
    "        doc = processor.process_file(test_file_path, title=f\"Test Doc - {strategy_name}\")\n",
    "        \n",
    "        results[strategy_name] = {\n",
    "            \"chunks\": len(doc.chunks),\n",
    "            \"avg_chunk_size\": sum(len(chunk.content) for chunk in doc.chunks) / len(doc.chunks),\n",
    "            \"total_content\": sum(len(chunk.content) for chunk in doc.chunks)\n",
    "        }\n",
    "        \n",
    "        print(f\"  ‚úÖ {strategy_name}: {len(doc.chunks)} chunks\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå {strategy_name} failed: {e}\")\n",
    "        results[strategy_name] = {\"error\": str(e)}\n",
    "\n",
    "# Summary\n",
    "print(\"\\nüìä Chunking Strategy Comparison:\")\n",
    "print(\"-\" * 60)\n",
    "for strategy, result in results.items():\n",
    "    if \"error\" in result:\n",
    "        print(f\"{strategy:12} | ERROR: {result['error']}\")\n",
    "    else:\n",
    "        print(f\"{strategy:12} | {result['chunks']:3d} chunks | Avg size: {result['avg_chunk_size']:6.1f} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Test 5: Factory Methods\n\nThis test evaluates the convenience factory methods provided by DocumentProcessor. These methods create pre-configured processors optimized for specific tasks like embeddings, Q&A, and semantic processing."
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üè≠ Testing Factory Methods:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulschmitt/miniforge3/envs/verbatim-rag-2/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ for_embeddings     |  56 chunks | Type: token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulschmitt/miniforge3/envs/verbatim-rag-2/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ for_qa             | 273 chunks | Type: sentence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulschmitt/miniforge3/envs/verbatim-rag-2/lib/python3.10/site-packages/chonkie/embeddings/auto.py:87: UserWarning: Failed to load minishlab/potion-base-8M with Model2VecEmbeddings: Model2VecEmbeddings.__init__() got an unexpected keyword argument 'merge_threshold'\n",
      "Falling back to loading default provider model.\n",
      "  warnings.warn(\n",
      "/Users/paulschmitt/miniforge3/envs/verbatim-rag-2/lib/python3.10/site-packages/chonkie/embeddings/auto.py:95: UserWarning: Failed to load the default model for Model2VecEmbeddings: Model2VecEmbeddings.__init__() got an unexpected keyword argument 'merge_threshold'\n",
      "Falling back to SentenceTransformerEmbeddings.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚ùå semantic           | Error: Failed to load embeddings via SentenceTransformerE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulschmitt/miniforge3/envs/verbatim-rag-2/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ markdown_recursive |  88 chunks | Type: recursive\n"
     ]
    }
   ],
   "source": [
    "# Test factory methods\n",
    "factory_methods = [\n",
    "    (\"for_embeddings\", DocumentProcessor.for_embeddings),\n",
    "    (\"for_qa\", DocumentProcessor.for_qa),\n",
    "    (\"semantic\", DocumentProcessor.semantic),\n",
    "    (\"markdown_recursive\", DocumentProcessor.markdown_recursive),\n",
    "]\n",
    "\n",
    "print(\"üè≠ Testing Factory Methods:\")\n",
    "for method_name, method in factory_methods:\n",
    "    try:\n",
    "        processor = method()\n",
    "        doc = processor.process_file(test_file_path, title=f\"Factory Test - {method_name}\")\n",
    "        print(f\"  ‚úÖ {method_name:18} | {len(doc.chunks):3d} chunks | Type: {processor.chunker_type}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå {method_name:18} | Error: {str(e)[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Test 6: Directory Processing\n\nThis test examines the DocumentProcessor's ability to process multiple files from a directory. It's useful for understanding batch processing capabilities and handling various file formats."
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Testing directory processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulschmitt/miniforge3/envs/verbatim-rag-2/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed 1 documents from directory\n",
      "  - VERBATIM_RAG_ACL: 88 chunks (pdf)\n"
     ]
    }
   ],
   "source": [
    "# Test directory processing if example docs exist\n",
    "if example_docs_path.exists():\n",
    "    print(\"üìÅ Testing directory processing...\")\n",
    "    try:\n",
    "        processor = DocumentProcessor()\n",
    "        documents = processor.process_directory(example_docs_path)\n",
    "        \n",
    "        print(f\"‚úÖ Processed {len(documents)} documents from directory\")\n",
    "        \n",
    "        for doc in documents:\n",
    "            print(f\"  - {doc.title}: {len(doc.chunks)} chunks ({doc.content_type.value})\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Directory processing failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"üìÅ Skipping directory test - no example docs found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Test 7: Document Structure Analysis\n\nThis critical test analyzes the document's structural patterns, including headers, sections, and hierarchical elements. The insights from this test inform our hierarchical chunking strategy and help identify documents suitable for hierarchical processing."
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Document Structure Analysis (for hierarchical chunking):\n",
      "\n",
      "Document: VERBATIM_RAG_ACL.pdf\n",
      "\n",
      "üìä Chunk Analysis:\n",
      "Chunk types distribution:\n",
      "  paragraph: 88\n",
      "\n",
      "Chunk size statistics:\n",
      "  Min: 1 chars\n",
      "  Max: 510 chars\n",
      "  Avg: 292.6 chars\n",
      "\n",
      "üå≥ Hierarchical Pattern Analysis:\n",
      "Found 18 headers:\n",
      "  Level 2: ## KR Labs at ArchEHR-QA 2025: A Verbatim Approach for Evidence-Based Question Answering\n",
      "  Level 2: ## Paul Schmitt\n",
      "  Level 2: ## Abstract\n",
      "  Level 2: ## 1 Introduction\n",
      "  Level 2: ## 2 Background\n",
      "  Level 2: ## 2.1 Dataset\n",
      "  Level 2: ## 2.2 Limitations of Standard RAG\n",
      "  Level 2: ## 2.3 Synthetic Training Data\n",
      "  Level 2: ## 3 Method\n",
      "  Level 2: ## 3.1 System Overview\n",
      "  Level 2: ## 3.2 Evidence Extraction\n",
      "  Level 2: ## 3.3 Synthetic Data Generation\n",
      "  Level 2: ## 3.4 Answer Generation\n",
      "  Level 2: ## 4 Evaluation\n",
      "  Level 2: ## 5 Ethical Considerations\n",
      "  Level 2: ## 6 Limitations\n",
      "  Level 2: ## 7 Conclusion\n",
      "  Level 2: ## References\n"
     ]
    }
   ],
   "source": [
    "# Analyze document structure for hierarchical chunking insights\n",
    "if 'document' in locals():\n",
    "    print(\"üîç Document Structure Analysis (for hierarchical chunking):\")\n",
    "    print(f\"\\nDocument: {document.title}\")\n",
    "    # print(f\"Raw content sample:\")\n",
    "    # print(document.raw_content[:500] + \"...\")\n",
    "    \n",
    "    print(f\"\\nüìä Chunk Analysis:\")\n",
    "    chunk_types = {}\n",
    "    chunk_sizes = []\n",
    "    \n",
    "    for chunk in document.chunks:\n",
    "        chunk_types[chunk.chunk_type] = chunk_types.get(chunk.chunk_type, 0) + 1\n",
    "        chunk_sizes.append(len(chunk.content))\n",
    "    \n",
    "    print(f\"Chunk types distribution:\")\n",
    "    for chunk_type, count in chunk_types.items():\n",
    "        print(f\"  {chunk_type.value}: {count}\")\n",
    "    \n",
    "    print(f\"\\nChunk size statistics:\")\n",
    "    print(f\"  Min: {min(chunk_sizes)} chars\")\n",
    "    print(f\"  Max: {max(chunk_sizes)} chars\")\n",
    "    print(f\"  Avg: {sum(chunk_sizes)/len(chunk_sizes):.1f} chars\")\n",
    "    \n",
    "    # Look for hierarchical patterns in content\n",
    "    print(f\"\\nüå≥ Hierarchical Pattern Analysis:\")\n",
    "    headers = []\n",
    "    for chunk in document.chunks:\n",
    "        lines = chunk.content.split('\\n')\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith('#'):\n",
    "                level = len(line) - len(line.lstrip('#'))\n",
    "                headers.append((level, line))\n",
    "    \n",
    "    if headers:\n",
    "        print(f\"Found {len(headers)} headers:\")\n",
    "        for level, header in headers:\n",
    "            indent = \"  \" * (level - 1)\n",
    "            print(f\"{indent}Level {level}: {header}\")\n",
    "    else:\n",
    "        print(\"No markdown headers found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Test 8: Document Serialization\n\nThis test verifies that documents can be properly serialized to and deserialized from dictionary format. This capability is essential for storing, transferring, and reconstructing document objects while maintaining data integrity."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test document serialization/deserialization\n",
    "if 'document' in locals():\n",
    "    print(\"üíæ Testing document serialization...\")\n",
    "    \n",
    "    try:\n",
    "        # Convert to dict\n",
    "        doc_dict = document.to_dict()\n",
    "        print(f\"‚úÖ Document serialized to dict ({len(str(doc_dict))} chars)\")\n",
    "        \n",
    "        # Convert back to document\n",
    "        restored_doc = document.__class__.from_dict(doc_dict)\n",
    "        print(f\"‚úÖ Document restored from dict\")\n",
    "        \n",
    "        # Verify integrity\n",
    "        print(f\"\\nüîç Integrity check:\")\n",
    "        print(f\"  Title match: {document.title == restored_doc.title}\")\n",
    "        print(f\"  Chunks count match: {len(document.chunks) == len(restored_doc.chunks)}\")\n",
    "        print(f\"  Content match: {document.raw_content == restored_doc.raw_content}\")\n",
    "        \n",
    "        if len(document.chunks) > 0 and len(restored_doc.chunks) > 0:\n",
    "            print(f\"  First chunk content match: {document.chunks[0].content == restored_doc.chunks[0].content}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Serialization test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Test 9: Docling HierarchicalChunker\n\nThis test explores Docling's built-in HierarchicalChunker to determine if it preserves document hierarchy better than standard chunking. We investigate whether the issue lies in PDF conversion or the chunking process itself."
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Test 11: Hierarchical Chunking Prototype\n\nThis is our breakthrough test that implements hierarchical chunking using section numbering patterns. It creates a complete hierarchical document structure with parent-child relationships, demonstrating how to overcome Docling's hierarchy flattening limitations through intelligent post-processing."
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Test 10: View Raw Converted Content\n\nThis test examines the raw markdown content produced by Docling's PDF conversion to understand exactly what happens during the document conversion process. It helps identify where hierarchy information is lost and explores alternative export methods."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 11: Hierarchical Chunking Prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Prototype hierarchical chunking using section numbering\n",
    "\n",
    "import re\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional\n",
    "from verbatim_rag.document import Chunk, ChunkType\n",
    "\n",
    "HEADER_RE = re.compile(r'^##\\s+(\\d+(?:\\.\\d+)*)\\s+(.+)$')\n",
    "NUM_RE    = re.compile(r'^(\\d+(?:\\.\\d+)*)\\s+([A-Z][A-Za-z\\s]+.*)$')\n",
    "\n",
    "@dataclass\n",
    "class HierarchicalChunk(Chunk):\n",
    "    \"\"\"Extended Chunk class with hierarchy support.\"\"\"\n",
    "    parent_chunk_id: Optional[str] = None\n",
    "    child_chunk_ids: List[str] = field(default_factory=list)\n",
    "    hierarchy_level: int = 0  # 0=document, 1=section, 2=subsection, 3=content\n",
    "    section_number: Optional[str] = None  # \"1\", \"2.1\", \"3.2.1\"\n",
    "\n",
    "    def add_child(self, child_chunk: 'HierarchicalChunk'):\n",
    "        \"\"\"Add a child chunk and set up parent-child relationship.\"\"\"\n",
    "        child_chunk.parent_chunk_id = self.id\n",
    "        if child_chunk.id not in self.child_chunk_ids:\n",
    "            self.child_chunk_ids.append(child_chunk.id)\n",
    "\n",
    "    def __str__(self):\n",
    "        indent = \"  \" * self.hierarchy_level\n",
    "        section = f\"[{self.section_number}] \" if self.section_number else \"\"\n",
    "        return f\"{indent}{section}{self.content[:100]}...\"\n",
    "\n",
    "def detect_section_numbering(content: str) -> List[tuple]:\n",
    "    \"\"\"\n",
    "    Detect section numbering patterns in content.\n",
    "    Returns list of (line_number, section_number, title, level, full_line)\n",
    "    \"\"\"\n",
    "    lines = content.split('\\n')\n",
    "    sections = []\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        line_stripped = line.strip()\n",
    "\n",
    "        # Pattern 1: \"## 1 Introduction\" or \"## 2.1 Dataset\"\n",
    "        match = HEADER_RE.match(line_stripped)\n",
    "        if match:\n",
    "            section_num = match.group(1)\n",
    "            title = match.group(2)\n",
    "            level = len(section_num.split('.'))\n",
    "            sections.append((i+1, section_num, title, level, line_stripped))\n",
    "            continue\n",
    "\n",
    "        # Pattern 2: Just numbers \"1 Introduction\" (without ##)\n",
    "        match = NUM_RE.match(line_stripped)\n",
    "        if match:\n",
    "            section_num = match.group(1)\n",
    "            title = match.group(2)\n",
    "            level = len(section_num.split('.'))\n",
    "            sections.append((i+1, section_num, title, level, line_stripped))\n",
    "\n",
    "    return sections\n",
    "\n",
    "def create_hierarchical_chunks(content: str, document_id: str) -> List[HierarchicalChunk]:\n",
    "    \"\"\"\n",
    "    Create hierarchical chunks from content using section numbering.\n",
    "    \"\"\"\n",
    "    # Step 1: Detect sections\n",
    "    sections = detect_section_numbering(content)\n",
    "\n",
    "    if not sections:\n",
    "        print(\"‚ùå No section numbering found - fallback to flat chunking\")\n",
    "        return []\n",
    "\n",
    "    print(f\"‚úÖ Found {len(sections)} sections with numbering\")\n",
    "\n",
    "    # Step 2: Split content by sections\n",
    "    lines = content.split('\\n')\n",
    "    hierarchical_chunks = []\n",
    "    chunk_map = {}  # section_number -> chunk\n",
    "\n",
    "    for i, (line_num, section_num, title, level, full_line) in enumerate(sections):\n",
    "        # Find content for this section (until next section)\n",
    "        start_line = line_num - 1  # Convert to 0-based\n",
    "        if i + 1 < len(sections):\n",
    "            end_line = sections[i + 1][0] - 1  # Next section's line\n",
    "        else:\n",
    "            end_line = len(lines)  # End of document\n",
    "\n",
    "        # Extract section content\n",
    "        section_lines = lines[start_line:end_line]\n",
    "        header, *body = section_lines\n",
    "        section_content = '\\n'.join(body).strip()\n",
    "\n",
    "        # Create hierarchical chunk\n",
    "        chunk = HierarchicalChunk(\n",
    "            document_id=document_id,\n",
    "            content=section_content,\n",
    "            chunk_number=i,\n",
    "            chunk_type=ChunkType.SECTION if level <= 2 else ChunkType.PARAGRAPH,\n",
    "            hierarchy_level=level,\n",
    "            section_number=section_num,\n",
    "            metadata={\n",
    "                'section_title': title,\n",
    "                'section_number': section_num,\n",
    "                'hierarchy_level': level\n",
    "            }\n",
    "        )\n",
    "\n",
    "        hierarchical_chunks.append(chunk)\n",
    "        chunk_map[section_num] = chunk\n",
    "\n",
    "    # Step 3: Build parent-child relationships\n",
    "    for chunk in hierarchical_chunks:\n",
    "        section_parts = chunk.section_number.split('.')\n",
    "\n",
    "        # Find parent (e.g., \"2.1\" parent is \"2\")\n",
    "        if len(section_parts) > 1:\n",
    "            parent_section = '.'.join(section_parts[:-1])\n",
    "            parent_chunk = chunk_map.get(parent_section)\n",
    "            if parent_chunk:\n",
    "                parent_chunk.add_child(chunk)\n",
    "\n",
    "    return hierarchical_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "outputs": [],
   "source": "## Test Summary and Recommendations\n\nThis final section summarizes all test results and provides actionable recommendations for implementing production-ready hierarchical chunking. It consolidates insights from all previous tests and outlines the next steps for development.",
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Analyze the detected Sections**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Section Detection Results:\n",
      "Found 14 sections:\n",
      "  Line  17: Level 1 - 1 Introduction\n",
      "  Line  30: Level 1 - 2 Background\n",
      "  Line  32:   Level 2 - 2.1 Dataset\n",
      "  Line  42:   Level 2 - 2.2 Limitations of Standard RAG\n",
      "  Line  46:   Level 2 - 2.3 Synthetic Training Data\n",
      "  Line  50: Level 1 - 3 Method\n",
      "  Line  52:   Level 2 - 3.1 System Overview\n",
      "  Line  56:   Level 2 - 3.2 Evidence Extraction\n",
      "  Line  66:   Level 2 - 3.3 Synthetic Data Generation\n",
      "  Line  79:   Level 2 - 3.4 Answer Generation\n",
      "  ... and 4 more sections\n"
     ]
    }
   ],
   "source": [
    "# Test section detection\n",
    "content = document.raw_content\n",
    "sections = detect_section_numbering(content)\n",
    "print(f\"üîç Section Detection Results:\")\n",
    "print(f\"Found {len(sections)} sections:\")\n",
    "\n",
    "for line_num, section_num, title, level, full_line in sections[:10]:\n",
    "    indent = \"  \" * (level - 1)\n",
    "    print(f\"  Line {line_num:3d}: {indent}Level {level} - {section_num} {title}\")\n",
    "\n",
    "if len(sections) > 10:\n",
    "    print(f\"  ... and {len(sections) - 10} more sections\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Hierarchical Chunks**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 14 sections with numbering\n",
      "Created 14 hierarchical chunks\n"
     ]
    }
   ],
   "source": [
    "hierarchical_chunks = create_hierarchical_chunks(content, document.id)\n",
    "print(f\"Created {len(hierarchical_chunks)} hierarchical chunks\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Analyze the Hierarchy**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "level_counts = {}\n",
    "parent_child_pairs = 0\n",
    "\n",
    "for chunk in hierarchical_chunks:\n",
    "    level_counts[chunk.hierarchy_level] = level_counts.get(chunk.hierarchy_level, 0) + 1\n",
    "    if chunk.child_chunk_ids:\n",
    "        parent_child_pairs += len(chunk.child_chunk_ids)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Level Distribution**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level distribution:\n",
      "  Level 1: 7 chunks\n",
      "  Level 2: 7 chunks\n",
      "Parent-child relationships: 7\n"
     ]
    }
   ],
   "source": [
    "print(f\"Level distribution:\")\n",
    "for level in sorted(level_counts.keys()):\n",
    "    print(f\"  Level {level}: {level_counts[level]} chunks\")\n",
    "print(f\"Parent-child relationships: {parent_child_pairs}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Hierarchy Structure**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "print(len(hierarchical_chunks))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Hierarchy Structure (first 10 chunks):\n",
      "\n",
      "Introduction\n",
      "Modern question-answering (QA) and retrievalaugmented generation (RAG) systems play a vital role in many high-stakes domains for information extraction and generation tasks. In medicine, a typical use case involves clinicians asking questions based on a patient's electronic health record (EHR) notes, rather than manually sifting through lengthy notes, which can be time-consuming. However, in practice, RAG and QA pipelines often misalign evidence and produce incorrect information, commonly referred to as hallucinations (Ji et al., 2023; Madsen et al., 2024). We argue that a reliable QA system should guarantee complete traceability of answers. To tackle this problem, we propose a verbatim pipeline that clearly separates extraction and generation to mitigate hallucinations (other errors may still occur):\n",
      "\n",
      "- Sentence-level extraction , using either zeroshot LLMs or supervised ModernBERT classifiers.\n",
      "- Template-constrained generation , dynamically creating answer templates filled exclu-\n",
      "\n",
      "We participated in the ArchEHR-QA 2025 shared task on grounded question answering (QA) from electronic health records (EHRs). Our approach involved (i) utilizing a zero-shot gemma-3-27b-it 1 LLM(Team, 2025) and (ii) generating synthetic data for sentence extraction from EHRs to train a compact extractor. For this purpose, we trained a Clinical ModernBERT classifier (Lee et al., 2025; Warner et al., 2024), achieving performance comparable to the LLM extractor. Both extractors were then fed into the same LLM template generator. Our solution achieved a score of 42.01% , ranking in the top 10 for core metrics, and surpassed the organizer's 70B-parameter Llama3.3 baseline by a large margin.\n",
      "\n",
      "Our contributions include a modular, traceable QA architecture that mitigates hallucinations, a method to generate synthetic EHR question-answer corpus and train custom models. Additionally, we are releasing all the code on GitHub 2 under the MIT License.\n",
      "\n",
      "The remainder of the paper discusses background (Section 2), method (Section 3), and evaluation (Section 4).\n",
      "\n",
      "Background\n",
      "\n",
      "     ‚îî‚îÄ‚îÄ Has 3 children\n",
      "\n",
      "Dataset\n",
      "Early clinical QA datasets such as emrQA (Pampari et al., 2018) and CliCR (≈†uster and Daelemans, 2018) used fill-in-the-blank methods and lacked explicit sentence-level evidence. ArchEHR-QA (Soni and Demner-Fushman, 2025b,a) addresses this by pairing clinician-authored questions with deidentified MIMIC-III (Johnson et al., 2016) notes,\n",
      "\n",
      "1 https://huggingface.co/google/gemma-3-27b-it\n",
      "\n",
      "2 https://github.com/KRLabsOrg/verbatim-rag/ tree/archehr\n",
      "\n",
      "annotated at the sentence-level as essential , supplementary , or irrelevant . Answers must be concise (under 75 words) and explicitly cite relevant sentences.\n",
      "\n",
      "Limitations of Standard RAG\n",
      "Standard RAG models, despite external grounding, still frequently hallucinate unsupported or contradictory information (Ji et al., 2023). Existing approaches like post-hoc verification (Friel and Sanyal, 2023; Manakul et al., 2023) or classifiers trained on hallucination corpora such as RAGTruth (Niu et al., 2024) (e.g., RAG-HAT (Song et al., 2024), LettuceDetect (√Åd√°m Kov√°cs and Recski, 2025)) add extra complexity and latency. Posthoc saliency methods (Serrano and Smith, 2019; Jain and Wallace, 2019) and LLM self-explanations (Madsen et al., 2024) have also been found unreliable. Our approach proactively prevents hallucinations through strict template-driven sentence extraction and verbatim insertion.\n",
      "\n",
      "Synthetic Training Data\n",
      "Due to limited access and annotation restrictions, obtaining sentence-level labeled clinical datasets is challenging. Recent works address this by generating synthetic data via perturbation or LLM prompting (Niu et al., 2024; Lozano et al., 2023; Frayling et al., 2024; Bai et al., 2024). We follow this approach, generating synthetic EHR snippets, clinician-style questions, and sentence relevance annotations (details in Section 3.3).\n",
      "\n",
      "Method\n",
      "\n",
      "     ‚îî‚îÄ‚îÄ Has 4 children\n",
      "\n",
      "System Overview\n",
      "Figure 1 depicts our system architecture. First, an extraction step identifies relevant sentences from the input (patient narrative, clinician question, and note excerpt). We implemented both zero-shot and supervised models. Second, the generation step uses gemma-3-27b-it to dynamically draft an answer template, filled verbatim with extracted sentences. If exceeding 75 words, answers are compressed via a summarization prompt, preserving sentence-level citations.\n",
      "\n",
      "Evidence Extraction\n",
      "We evaluated two extractors: (i) We prompted gemma-3-27b-it to explicitly label sentences as relevant via a step-by-step process. (ii) We finetuned a Clinical ModernBERT classifier (Lee et al.,\n",
      "\n",
      "Figure 1: System overview. The pipeline first selects relevant sentences and then generates a question-specific answer using a dynamic template.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "2025), trained on our synthetic data (Section 3.3). It independently evaluates each sentence in context (question + patient narrative). Lee et al. (2025) is a variant of ModernBERT (Warner et al., 2024) adapted specifically for biomedical and clinical text. Clinical ModernBERT supports extended input sequences (up to 8,192 tokens) and includes domain-specific vocabulary enhancements, making it particularly suitable for handling long clinical narratives. To provide additional context during classification, we included one sentence before and after the target sentence, forming a passage of up to three sentences. We chose a window size of one sentence before and after the target based on preliminary experimentation. The target sentence was explicitly marked with [START] and [END] tokens. The full input was structured using the standard BERT classification format. During fine-tuning, we merged essential and supplementary labels into a single positive class. We addressed class imbalance using weighted binary cross-entropy loss. We trained for 3 epochs (batch size 32, learning rate 2e-5), with gradient clipping and early stopping based on F1 score.\n",
      "\n",
      "Synthetic Data Generation\n",
      "Due to the scarcity of publicly available annotated data for sentence-level relevance classification, we constructed a synthetic dataset tailored specifically to the ArchEHR-QA task. Although the official development set contains labeled sentences, it is lim- ited to 428 sentences across only 20 question-note pairs. Initial experiments using external resources like RAGBench (Friel et al., 2025) and PubMedQAderived corpora (Jin et al., 2019) showed poor transfer performance, emphasizing the need for taskspecific synthetic data.\n",
      "\n",
      "Wegenerated synthetic data via few-shot prompting with gemma-3-27b-it . Each prompt provided dynamic examples from the development set to ensure diversity. The LLM generated synthetic instances comprising de-identified clinical note excerpts, patient narratives, clinician-authored questions, and binary relevance labels. This approach yielded 3915 synthetic notes. We varied the few-shot examples across multiple runs, as static prompting resulted in repetitive outputs. This variation greatly increased lexical and semantic diversity, aligning with other work in synthetic data generation (Li et al., 2023; Tang et al., 2023; Xu et al., 2024). Ultimately, selecting each sentence with their relevance from the note excerpts, we constructed a comprehensive dataset of 58k synthetic training examples, each labeled at the sentence level, which formed the training set for our Clinical ModernBERT classifier. Table 1 shows an illustrative training instance.\n",
      "\n",
      "Table 1: An example model input for our training.\n",
      "\n",
      "| QUESTION   | Patient narrative : My husband, a 72-year-old with a history of COPD, was admitted for worsening shortness of breath. He's been on home oxygen for years, but it wasn't helping this time. He also developed some swelling in his ankles. He seems a little confused today... Clinician question : What is the likely cause of the patient's ankle edema and what was done to address it?                          |\n",
      "|------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| SENTENCE   | Adiuretic, furosemide 40mg PO daily, was initiated to address the lower extremity edema, which was at- tributed to both underlying heart failure and fluid re- tention secondary to COPD exacerbation. [START] Echocardiogram revealed mild left ventricular dys- function with an estimated ejection fraction of 45%. [END] Renal function was monitored closely, and remained stable throughout hospitalization. |\n",
      "| LABEL      | RELEVANT                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "\n",
      "Answer Generation\n",
      "The answer generation module dynamically creates a template using the LLM ( gemma-3-27b-it ) based on the clinician's question, the selected evidence sentences, and the clinical note context. After the template generation step, we directly insert the extracted evidence sentences verbatim into the generated template, referencing sentence IDs ex-\n",
      "\n",
      "The emergency salvage repair was performed due to: -He was transferred to the hospital on 2025-01-20 for emergent repair of his ruptured thoracoabdominal aortic aneurysm. |1| -He was immediately taken to the operating room where he underwent an emergent salvage repair of ruptured thoracoabdominal aortic aneurysm with a 34-mm Dacron tube graft using deep hypothermic circulatory arrest. |2| -Thoracoabdominal wound healing well with exception of very small open area mid-wound that is ~1 cm around and 0.5 cm deep, no surrounding erythema. |8|\n",
      "\n",
      "Figure 2: Example answer generated by our verbatim method, inserting evidence sentences verbatim into a dynamically generated template.\n",
      "\n",
      "He was transferred to the hospital on 2025-01-20 for emergent repair of his ruptured thoracoabdominal aortic aneurysm |1|. He underwent an emergent salvage repair with a 34-mm Dacron tube graft using deep hypothermic circulatory arrest |2|. See also: |8|\n",
      "\n",
      "Figure 3: Concise answer produced by our summarization step to comply with the 75-word limit.\n",
      "\n",
      "plicitly. An example filled template generated by our pipeline is shown in Figure 2.\n",
      "\n",
      "If the filled answer exceeds the 75-word constraint of the task, we use an additional summarization prompt to rewrite the answer more concisely, ensuring all selected evidence remains cited and intact. An example summarization of the answer from Figure 2 is illustrated in Figure 3.\n"
     ]
    }
   ],
   "source": [
    "print(f\"üìã Hierarchy Structure (first 10 chunks):\")\n",
    "for i, chunk in enumerate(hierarchical_chunks[:10]):\n",
    "    print()\n",
    "    #print(f\"{i+1:2d}. {chunk}\")\n",
    "    print(chunk.metadata[\"section_title\"])\n",
    "    print(chunk.content)\n",
    "    if chunk.child_chunk_ids:\n",
    "        print(f\"     ‚îî‚îÄ‚îÄ Has {len(chunk.child_chunk_ids)} children\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'section_title': 'Introduction', 'section_number': '1', 'hierarchy_level': 1}\n",
      "Modern question-answering (QA) and retrievalaugmented generation (RAG) systems play a vital role in many high-stakes domains for information extraction and generation tasks. In medicine, a typical use case involves clinicians asking questions based on a patient's electronic health record (EHR) notes, rather than manually sifting through lengthy notes, which can be time-consuming. However, in practice, RAG and QA pipelines often misalign evidence and produce incorrect information, commonly referred to as hallucinations (Ji et al., 2023; Madsen et al., 2024). We argue that a reliable QA system should guarantee complete traceability of answers. To tackle this problem, we propose a verbatim pipeline that clearly separates extraction and generation to mitigate hallucinations (other errors may still occur):\n",
      "\n",
      "- Sentence-level extraction , using either zeroshot LLMs or supervised ModernBERT classifiers.\n",
      "- Template-constrained generation , dynamically creating answer templates filled exclu-\n",
      "\n",
      "We participated in the ArchEHR-QA 2025 shared task on grounded question answering (QA) from electronic health records (EHRs). Our approach involved (i) utilizing a zero-shot gemma-3-27b-it 1 LLM(Team, 2025) and (ii) generating synthetic data for sentence extraction from EHRs to train a compact extractor. For this purpose, we trained a Clinical ModernBERT classifier (Lee et al., 2025; Warner et al., 2024), achieving performance comparable to the LLM extractor. Both extractors were then fed into the same LLM template generator. Our solution achieved a score of 42.01% , ranking in the top 10 for core metrics, and surpassed the organizer's 70B-parameter Llama3.3 baseline by a large margin.\n",
      "\n",
      "Our contributions include a modular, traceable QA architecture that mitigates hallucinations, a method to generate synthetic EHR question-answer corpus and train custom models. Additionally, we are releasing all the code on GitHub 2 under the MIT License.\n",
      "\n",
      "The remainder of the paper discusses background (Section 2), method (Section 3), and evaluation (Section 4).\n"
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(hierarchical_chunks[:10]):\n",
    "    print(chunk.metadata)\n",
    "    print(chunk.content)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Test: Find children of a parent**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üë®‚Äçüëß‚Äçüë¶ Parent-Child Relationship Test:\n",
      "Parent: 2 Background\n",
      "  ‚îî‚îÄ‚îÄ Child: 2.1 Dataset\n",
      "  ‚îî‚îÄ‚îÄ Child: 2.2 Limitations of Standard RAG\n",
      "  ‚îî‚îÄ‚îÄ Child: 2.3 Synthetic Training Data\n"
     ]
    }
   ],
   "source": [
    "print(f\"üë®‚Äçüëß‚Äçüë¶ Parent-Child Relationship Test:\")\n",
    "for chunk in hierarchical_chunks[:5]:\n",
    "    if chunk.child_chunk_ids:\n",
    "        print(f\"Parent: {chunk.section_number} {chunk.metadata.get('section_title', '')}\")\n",
    "        for child_id in chunk.child_chunk_ids:\n",
    "            child_chunk = next((c for c in hierarchical_chunks if c.id == child_id), None)\n",
    "            if child_chunk:\n",
    "                print(f\"  ‚îî‚îÄ‚îÄ Child: {child_chunk.section_number} {child_chunk.metadata.get('section_title', '')}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 9: Docling HierarchicalChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deep Analysis of Docling DocChunks:\n",
      "\n",
      "--- DocChunk 1 Deep Dive ---\n",
      "Raw text (36 chars):\n",
      "'√Åd√°m Kov√°cs KR Labs kovacs@krlabs.eu...'\n",
      "Meta type: <class 'docling_core.transforms.chunker.hierarchical_chunker.DocMeta'>\n",
      "Doc items count: 1\n",
      "  Item 0: TextItem\n",
      "    Text: '√Åd√°m Kov√°cs KR Labs kovacs@krlabs.eu...'\n",
      "    Parent: cref='#/body'\n",
      "\n",
      "--- DocChunk 2 Deep Dive ---\n",
      "Raw text (33 chars):\n",
      "'TU Wien paul.schmitt@tuwien.ac.at...'\n",
      "Meta type: <class 'docling_core.transforms.chunker.hierarchical_chunker.DocMeta'>\n",
      "Doc items count: 1\n",
      "  Item 0: TextItem\n",
      "    Text: 'TU Wien paul.schmitt@tuwien.ac.at...'\n",
      "    Parent: cref='#/body'\n",
      "\n",
      "--- DocChunk 3 Deep Dive ---\n",
      "Raw text (45 chars):\n",
      "'G√°bor Recski KR Labs TU Wien recski@krlabs.eu...'\n",
      "Meta type: <class 'docling_core.transforms.chunker.hierarchical_chunker.DocMeta'>\n",
      "Doc items count: 1\n",
      "  Item 0: TextItem\n",
      "    Text: 'G√°bor Recski KR Labs TU Wien recski@krlabs.eu...'\n",
      "    Parent: cref='#/body'\n",
      "\n",
      "--- DocChunk 4 Deep Dive ---\n",
      "Raw text (678 chars):\n",
      "'We present a lightweight, domain-agnostic verbatim pipeline for evidence-grounded question answering. Our pipeline operates in two steps: first, a sentence-level extractor flags relevant note sentences using either zero-shot LLM prompts or supervised ModernBERT classifiers. Next, an LLM drafts a que...'\n",
      "Meta type: <class 'docling_core.transforms.chunker.hierarchical_chunker.DocMeta'>\n",
      "Doc items count: 1\n",
      "  Item 0: TextItem\n",
      "    Text: 'We present a lightweight, domain-agnostic verbatim pipeline for evidence-grounded question answering...'\n",
      "    Parent: cref='#/body'\n",
      "\n",
      "--- DocChunk 5 Deep Dive ---\n",
      "Raw text (66 chars):\n",
      "'sively with verbatim sentences selected from the extraction phase....'\n",
      "Meta type: <class 'docling_core.transforms.chunker.hierarchical_chunker.DocMeta'>\n",
      "Doc items count: 1\n",
      "  Item 0: TextItem\n",
      "    Text: 'sively with verbatim sentences selected from the extraction phase....'\n",
      "    Parent: cref='#/body'\n",
      "\n",
      "üî¢ Section Number Analysis:\n",
      "‚ùå No clear section numbering patterns found\n",
      "May need to use content-based or semantic hierarchy\n"
     ]
    }
   ],
   "source": [
    "# Follow-up: Examine DocChunk structure and content\n",
    "if 'chunks' in locals() and chunks:\n",
    "    print(\"üîç Deep Analysis of Docling DocChunks:\")\n",
    "    \n",
    "    for i, chunk in enumerate(chunks[:5]):\n",
    "        print(f\"\\n--- DocChunk {i+1} Deep Dive ---\")\n",
    "        \n",
    "        # Access the text content directly\n",
    "        chunk_text = chunk.text if hasattr(chunk, 'text') else str(chunk)\n",
    "        print(f\"Raw text ({len(chunk_text)} chars):\")\n",
    "        print(f\"'{chunk_text[:300]}...'\")\n",
    "        \n",
    "        # Check chunk metadata\n",
    "        if hasattr(chunk, 'meta'):\n",
    "            print(f\"Meta type: {type(chunk.meta)}\")\n",
    "            if hasattr(chunk.meta, 'doc_items'):\n",
    "                print(f\"Doc items count: {len(chunk.meta.doc_items)}\")\n",
    "                \n",
    "                # Look at document items for structure\n",
    "                for j, item in enumerate(chunk.meta.doc_items[:3]):\n",
    "                    print(f\"  Item {j}: {type(item).__name__}\")\n",
    "                    if hasattr(item, 'text'):\n",
    "                        print(f\"    Text: '{item.text[:100]}...'\")\n",
    "                    if hasattr(item, 'parent'):\n",
    "                        print(f\"    Parent: {item.parent}\")\n",
    "                    if hasattr(item, 'level') or hasattr(item, 'hierarchy_level'):\n",
    "                        level = getattr(item, 'level', getattr(item, 'hierarchy_level', None))\n",
    "                        print(f\"    Level: {level}\")\n",
    "        \n",
    "        # Look for section patterns in the text\n",
    "        lines = chunk_text.split('\\n')\n",
    "        for line_num, line in enumerate(lines[:10]):\n",
    "            line = line.strip()\n",
    "            # Look for section patterns like \"1 Introduction\", \"2.1 Dataset\"\n",
    "            import re\n",
    "            if re.match(r'^\\d+(\\.\\d+)*\\s+[A-Z]', line):\n",
    "                print(f\"  üìç Section pattern found: '{line}'\")\n",
    "            elif re.match(r'^[A-Z][A-Za-z\\s]+$', line) and len(line) < 50:\n",
    "                print(f\"  üìù Possible header: '{line}'\")\n",
    "\n",
    "    # Test: Can we detect hierarchy from section numbering?\n",
    "    print(f\"\\nüî¢ Section Number Analysis:\")\n",
    "    section_patterns = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        chunk_text = chunk.text if hasattr(chunk, 'text') else str(chunk)\n",
    "        lines = chunk_text.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            # Match patterns like \"1 Introduction\", \"2.1 Dataset\", \"2.1.1 Details\"\n",
    "            match = re.match(r'^(\\d+(?:\\.\\d+)*)\\s+([A-Z][A-Za-z\\s]+)', line)\n",
    "            if match:\n",
    "                section_num = match.group(1)\n",
    "                section_title = match.group(2)\n",
    "                level = len(section_num.split('.'))\n",
    "                section_patterns.append((level, section_num, section_title))\n",
    "    \n",
    "    if section_patterns:\n",
    "        print(\"‚úÖ Found section number hierarchy:\")\n",
    "        for level, num, title in section_patterns[:10]:\n",
    "            indent = \"  \" * (level - 1)\n",
    "            print(f\"{indent}Level {level}: {num} {title}\")\n",
    "        \n",
    "        print(f\"\\nüöÄ SOLUTION: Use section numbering for hierarchy!\")\n",
    "        print(\"We can create hierarchical chunks based on section numbers:\")\n",
    "        print(\"  Level 1: 1, 2, 3, 4...\")\n",
    "        print(\"  Level 2: 2.1, 2.2, 3.1...\")\n",
    "        print(\"  Level 3: 2.1.1, 2.1.2...\")\n",
    "    else:\n",
    "        print(\"‚ùå No clear section numbering patterns found\")\n",
    "        print(\"May need to use content-based or semantic hierarchy\")\n",
    "else:\n",
    "    print(\"‚ùå No chunks available for deep analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 9.5: Docling Conversion Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Examining Raw Docling Conversion:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulschmitt/miniforge3/envs/verbatim-rag-2/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Method 1: Standard export_to_markdown():\n",
      "Length: 25746 characters\n",
      "First 2000 characters:\n",
      "------------------------------------------------------------\n",
      "\"## KR Labs at ArchEHR-QA 2025: A Verbatim Approach for Evidence-Based Question Answering\\n\\n√Åd√°m Kov√°cs KR Labs kovacs@krlabs.eu\\n\\n## Paul Schmitt\\n\\nTU Wien paul.schmitt@tuwien.ac.at\\n\\nG√°bor Recski KR Labs TU Wien recski@krlabs.eu\\n\\n## Abstract\\n\\nWe present a lightweight, domain-agnostic verbatim pipeline for evidence-grounded question answering. Our pipeline operates in two steps: first, a sentence-level extractor flags relevant note sentences using either zero-shot LLM prompts or supervised ModernBERT classifiers. Next, an LLM drafts a questionspecific template, which is filled verbatim with sentences from the extraction step. This prevents hallucinations and ensures traceability. In the ArchEHR-QA 2025 shared task, our system scored 42.01%, ranking top-10 in core metrics and outperforming the organiser's 70B-parameter Llama-3.3 baseline. We publicly release our code and inference scripts under an MIT license.\\n\\nsively with verbatim sentences selected from the extraction phase.\\n\\n## 1 Introduction\\n\\nModern question-answering (QA) and retrievalaugmented generation (RAG) systems play a vital role in many high-stakes domains for information extraction and generation tasks. In medicine, a typical use case involves clinicians asking questions based on a patient's electronic health record (EHR) notes, rather than manually sifting through lengthy notes, which can be time-consuming. However, in practice, RAG and QA pipelines often misalign evidence and produce incorrect information, commonly referred to as hallucinations (Ji et al., 2023; Madsen et al., 2024). We argue that a reliable QA system should guarantee complete traceability of answers. To tackle this problem, we propose a verbatim pipeline that clearly separates extraction and generation to mitigate hallucinations (other errors may still occur):\\n\\n- Sentence-level extraction , using either zeroshot LLMs or supervised ModernBERT classifiers.\\n- Template-constrained generation , dynamically creating answer templates filled exc\"\n",
      "------------------------------------------------------------\n",
      "\n",
      "üîç Method 2: Available export methods:\n",
      "Available methods: ['_export_to_indented_text', 'export_to_dict', 'export_to_doctags', 'export_to_document_tokens', 'export_to_element_tree', 'export_to_html', 'export_to_markdown', 'export_to_text']\n",
      "\n",
      "üîç Method 3: Document structure:\n",
      "Document type: <class 'docling_core.types.doc.document.DoclingDocument'>\n",
      "Document attributes: ['add_code', 'add_document', 'add_form', 'add_formula', 'add_group', 'add_heading', 'add_inline_group', 'add_key_values', 'add_list_group', 'add_list_item']...\n",
      "\n",
      "üîç Method 4: Document body structure:\n",
      "Body type: <class 'docling_core.types.doc.document.GroupItem'>\n",
      "Body attributes: ['children', 'construct', 'content_layer', 'copy', 'dict', 'from_orm', 'get_ref', 'json', 'label', 'model_computed_fields']...\n",
      "\n",
      "üíæ Saved full content to: /Users/paulschmitt/DataspellProjects/verbatim-rag/debug_converted_content.md\n",
      "You can open this file to see the complete converted content!\n",
      "\n",
      "üîç Method 6: Header pattern analysis in raw content:\n",
      "Found 18 potential headers:\n",
      "  Line 1: ## KR Labs at ArchEHR-QA 2025: A Verbatim Approach for Evidence-Based Question Answering\n",
      "  Line 5: ## Paul Schmitt\n",
      "  Line 11: ## Abstract\n",
      "  Line 17: ## 1 Introduction\n",
      "  Line 30: ## 2 Background\n",
      "  Line 32: ## 2.1 Dataset\n",
      "  Line 42: ## 2.2 Limitations of Standard RAG\n",
      "  Line 46: ## 2.3 Synthetic Training Data\n",
      "  Line 50: ## 3 Method\n",
      "  Line 52: ## 3.1 System Overview\n",
      "  Line 56: ## 3.2 Evidence Extraction\n",
      "  Line 66: ## 3.3 Synthetic Data Generation\n",
      "  Line 79: ## 3.4 Answer Generation\n",
      "  Line 95: ## 4 Evaluation\n",
      "  Line 124: ## 5 Ethical Considerations\n",
      "  ... and 3 more\n"
     ]
    }
   ],
   "source": [
    "# View the raw converted markdown content to see what Docling actually produces\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "print(\"üìÑ Examining Raw Docling Conversion:\")\n",
    "\n",
    "pdf_path = project_root / \"data\" / \"acl_papers\" / \"VERBATIM_RAG_ACL.pdf\"\n",
    "\n",
    "if pdf_path.exists():\n",
    "    converter = DocumentConverter()\n",
    "    result = converter.convert(str(pdf_path))\n",
    "    \n",
    "    # Method 1: Standard markdown export (what DocumentProcessor uses)\n",
    "    print(\"\\nüîç Method 1: Standard export_to_markdown():\")\n",
    "    markdown_content = result.document.export_to_markdown()\n",
    "    print(f\"Length: {len(markdown_content)} characters\")\n",
    "    print(\"First 2000 characters:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(repr(markdown_content[:2000]))  # Use repr to see actual characters\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Method 2: Check if there are other export options\n",
    "    print(f\"\\nüîç Method 2: Available export methods:\")\n",
    "    export_methods = [method for method in dir(result.document) if 'export' in method.lower()]\n",
    "    print(f\"Available methods: {export_methods}\")\n",
    "    \n",
    "    # Method 3: Look at document structure\n",
    "    print(f\"\\nüîç Method 3: Document structure:\")\n",
    "    print(f\"Document type: {type(result.document)}\")\n",
    "    doc_attrs = [attr for attr in dir(result.document) if not attr.startswith('_')]\n",
    "    print(f\"Document attributes: {doc_attrs[:10]}...\")\n",
    "    \n",
    "    # Method 4: Try to access raw document elements\n",
    "    if hasattr(result.document, 'body'):\n",
    "        print(f\"\\nüîç Method 4: Document body structure:\")\n",
    "        print(f\"Body type: {type(result.document.body)}\")\n",
    "        body_attrs = [attr for attr in dir(result.document.body) if not attr.startswith('_')]\n",
    "        print(f\"Body attributes: {body_attrs[:10]}...\")\n",
    "    \n",
    "    # Method 5: Save to file for inspection\n",
    "    output_file = project_root / \"debug_converted_content.md\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(markdown_content)\n",
    "    print(f\"\\nüíæ Saved full content to: {output_file}\")\n",
    "    print(\"You can open this file to see the complete converted content!\")\n",
    "    \n",
    "    # Method 6: Look for header patterns in the raw content\n",
    "    print(f\"\\nüîç Method 6: Header pattern analysis in raw content:\")\n",
    "    lines = markdown_content.split('\\n')\n",
    "    header_lines = []\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        line_stripped = line.strip()\n",
    "        if line_stripped.startswith('#'):\n",
    "            header_lines.append((i+1, line))\n",
    "        elif re.match(r'^\\d+(\\.\\d+)*\\s+[A-Z]', line_stripped):\n",
    "            header_lines.append((i+1, f\"[NUMBER] {line}\"))\n",
    "    \n",
    "    print(f\"Found {len(header_lines)} potential headers:\")\n",
    "    for line_num, header in header_lines[:15]:  # Show first 15\n",
    "        print(f\"  Line {line_num}: {header}\")\n",
    "    \n",
    "    if len(header_lines) > 15:\n",
    "        print(f\"  ... and {len(header_lines) - 15} more\")\n",
    "\n",
    "else:\n",
    "    print(f\"‚ùå PDF not found: {pdf_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 10: View Raw Converted Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Test Summary:\n",
      "==================================================\n",
      "\n",
      "‚úÖ Successfully tested:\n",
      "  - Basic DocumentProcessor creation\n",
      "  - File processing with different chunking strategies\n",
      "  - Factory methods for specialized processors\n",
      "  - Document structure analysis\n",
      "  - Document serialization/deserialization\n",
      "\n",
      "üöÄ Ready for hierarchical chunking implementation!\n",
      "\n",
      "Next steps for hierarchical chunking:\n",
      "  1. Extend Chunk class with parent_chunk_id field\n",
      "  2. Modify DocumentProcessor to create hierarchical relationships\n",
      "  3. Update VerbatimIndex to handle hierarchical chunks\n",
      "  4. Add hierarchical search capabilities\n",
      "\n",
      "üí° Insights for hierarchical chunking:\n",
      "  - Document has 18 headers for natural hierarchy\n",
      "  - Maximum header depth: 2 levels\n",
      "  - Can use markdown structure for parent-child relationships\n",
      "\n",
      "üßπ Cleanup...\n",
      "Removed test file: /Users/paulschmitt/DataspellProjects/verbatim-rag/data/acl_papers/VERBATIM_RAG_ACL.pdf\n"
     ]
    }
   ],
   "source": [
    "print(\"üìã Test Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\n‚úÖ Successfully tested:\")\n",
    "print(\"  - Basic DocumentProcessor creation\")\n",
    "print(\"  - File processing with different chunking strategies\")\n",
    "print(\"  - Factory methods for specialized processors\")\n",
    "print(\"  - Document structure analysis\")\n",
    "print(\"  - Document serialization/deserialization\")\n",
    "\n",
    "print(\"\\nüöÄ Ready for hierarchical chunking implementation!\")\n",
    "print(\"\\nNext steps for hierarchical chunking:\")\n",
    "print(\"  1. Extend Chunk class with parent_chunk_id field\")\n",
    "print(\"  2. Modify DocumentProcessor to create hierarchical relationships\")\n",
    "print(\"  3. Update VerbatimIndex to handle hierarchical chunks\")\n",
    "print(\"  4. Add hierarchical search capabilities\")\n",
    "\n",
    "print(\"\\nüí° Insights for hierarchical chunking:\")\n",
    "if 'headers' in locals() and headers:\n",
    "    print(f\"  - Document has {len(headers)} headers for natural hierarchy\")\n",
    "    max_level = max(level for level, _ in headers)\n",
    "    print(f\"  - Maximum header depth: {max_level} levels\")\n",
    "    print(\"  - Can use markdown structure for parent-child relationships\")\n",
    "else:\n",
    "    print(\"  - Document lacks clear hierarchical structure\")\n",
    "    print(\"  - Consider semantic-based or size-based hierarchical chunking\")\n",
    "\n",
    "print(\"\\nüßπ Cleanup...\")\n",
    "if test_file_path.exists():\n",
    "    test_file_path.unlink()\n",
    "    print(f\"Removed test file: {test_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
