{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dda9d14-09c0-4142-88e9-562a9778a6a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac55dbe9-f0aa-4400-82cc-1034d86bf718",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402956f4-d20c-4a9c-9965-a263e3d45ee9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VLLM_URL = \"http://localhost:8000/v1/completions\"\n",
    "PROMPTS_DIR = Path(\"prompts\")\n",
    "OUTPUT_DIR = Path(\"../data/synthetic/note-excerpts\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90c2649-71a5-4f0a-998c-8fbf85874b6c",
   "metadata": {},
   "source": [
    "# Build Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09782b1b-c031-4c7a-ba34-ce4dbc537601",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Define Prompt Template**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260b0874-dff8-4d0a-bdfb-f53842724c67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROMPT_TMPL = \"\"\"Only output one paragraph—no comments, no lists.  Begin with exactly one bold header (choose one):\n",
    "\n",
    "**Brief Hospital Course:**  \n",
    "**Major Procedures:**  \n",
    "**Discharge Summary:**\n",
    "\n",
    "Then, in approximately {n_sentences} sentences or sentence fragments, document a realistic hospital note excerpt in a rushed, semi‑structured style that covers:\n",
    "- admission reason\n",
    "- key findings (labs/imaging/procedures)\n",
    "- interventions/treatments\n",
    "- discharge plan\n",
    "\n",
    "Include common EHR shorthand (HTN, DM2, CAD, WNL, BP, HR, SpO2, CXR, CT scan, ICU, OR, PO, IV, BiPAP).  \n",
    "Inject “noise” (~10% of sentences) by omitting commas, double‑spacing words, or dangling fragments.  \n",
    "Vary sentence length (2–3 words up to ~25 words).  \n",
    "Use demographics only as “A ##‑year‑old M” or “A ##‑year‑old F” inside a sentence—never full identifiers. \n",
    "Focus on: {scenario}\n",
    "\n",
    "End with ***END NOTE***\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec527748-bd25-4654-b4c4-5eb5d635ba84",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Candidates: Headers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3974ec7-14da-4819-8a84-215dcc0c82a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HEADERS = [\n",
    "    \"**Brief Hospital Course:**\",\n",
    "    \"**Major Procedures:**\",\n",
    "    \"**Discharge Summary:**\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00854ee1-753c-46f1-8927-b725c9a9e6c0",
   "metadata": {},
   "source": [
    "**Candidates: Scenario**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30569eed-cb62-47ca-8ea5-7f916579a3fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SCENARIOS = [\n",
    "  \"COPD exacerbation in a smoker\",\n",
    "  \"Community‑acquired pneumonia\",\n",
    "  \"Acute decompensated heart failure\",\n",
    "  \"STEMI post‑PCI\",\n",
    "  \"Ischemic stroke on tPA\",\n",
    "  \"DKA in type 1 diabetic\",\n",
    "  \"GI bleed from PUD\",\n",
    "  \"Sepsis from UTI\",\n",
    "  \"Post‑op hip fracture repair\",\n",
    "  \"AKI on CKD\",\n",
    "  \"Preeclampsia in 3rd trimester\",\n",
    "  \"Liver transplant post‑op day 2\",\n",
    "  \"Traumatic brain injury\",\n",
    "  \"ARDS on ventilator\",\n",
    "  \"Vascular surgery post‑op\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed8f6aa-d2db-48e8-b6d0-480eff6f034e",
   "metadata": {},
   "source": [
    "**Candidates: Number of Sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65db0562-49d2-44db-a8d5-97d96d3bb51e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket = random.choice([(8, 9), (10, 12), (14, 17)])\n",
    "n_sentences = random.randint(*bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48050c8a-51ce-444a-824d-a37cdd033e8c",
   "metadata": {},
   "source": [
    "**Building the prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997d5533-e53f-4473-b181-83525ed877e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_prompt(template, scenarios):\n",
    "    bucket = random.choice([(8, 9), (10, 12), (14, 17)])\n",
    "    n_sentences = random.randint(*bucket)\n",
    "    scenario = random.choice(scenarios)\n",
    "\n",
    "    return PROMPT_TMPL.format(\n",
    "        scenario=scenario,\n",
    "        n_sentences = n_sentences\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1626d6e8-7483-47cb-adaa-9261a898d089",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "make_prompt(PROMPT_TMPL, SCENARIOS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9891feb-4888-4779-a0d4-00d342ff26cb",
   "metadata": {},
   "source": [
    "# Generation functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc4fa17-717f-494a-85b8-59021e6591c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_from_vllm(\n",
    "    prompt: str,\n",
    "    temperature: float = 0.7,\n",
    "    max_tokens: int = 1024,\n",
    "    top_p: float = 0.95,\n",
    "    seed: int = None,\n",
    "    retries: int = 3,\n",
    "    delay: int = 2,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Send prompt to local vLLM server and return the generated text.\n",
    "    \n",
    "    Args:\n",
    "      prompt: The text prompt to complete.\n",
    "      temperature: Sampling temperature.\n",
    "      max_tokens: Maximum number of tokens to generate.\n",
    "      top_p: Nucleus sampling cutoff.\n",
    "      seed: Optional RNG seed to get deterministic outputs.\n",
    "      stop_sequences: Optional list of strings; generation will stop before any of them.\n",
    "      retries: How many times to retry on failure.\n",
    "      delay: Seconds to wait between retries.\n",
    "    Returns:\n",
    "      The completed text (empty string if all attempts failed).\n",
    "    \"\"\"\n",
    "    payload: dict[str, any] = {\n",
    "        \"prompt\": prompt,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"top_p\": top_p,\n",
    "    }\n",
    "    if seed is not None:\n",
    "        payload[\"seed\"] = seed\n",
    "    \n",
    "    payload[\"stop_sequences\"] = [\"***END NOTE***\"]\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            r = requests.post(VLLM_URL, json=payload)\n",
    "            r.raise_for_status()\n",
    "            text = r.json()[\"choices\"][0][\"text\"]\n",
    "            return text.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"[Warning] vLLM call failed (attempt {attempt}): {e}\")\n",
    "            time.sleep(delay)\n",
    "\n",
    "    print(\"[Error] All vLLM attempts failed, returning empty string.\")\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bdd95c-5a64-4fbf-8cad-06ceb4fb274f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_end_tag(raw_output: str, end_token: str = \"***END NOTE***\") -> str:\n",
    "    \"\"\"\n",
    "    Extracts the clinical note from a raw LLM response by\n",
    "    removing everything starting with the end_token.\n",
    "    \"\"\"\n",
    "    # Find the position of the end token\n",
    "    idx = raw_output.find(end_token)\n",
    "    if idx != -1:\n",
    "        # Return everything before the end token, stripped of extra whitespace\n",
    "        return raw_output[:idx].strip()\n",
    "    # If no end token found, just return the trimmed raw output\n",
    "    return raw_output.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fbfba9-4355-4a83-a189-6c2593c6f18f",
   "metadata": {},
   "source": [
    "# Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba4f5ca-f07a-4bf0-a284-b28ccf7cc53b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f52878-581a-4a7b-81b8-d1373f143972",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "make_prompt(PROMPT_TMPL, SCENARIOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bb0235-38c1-493b-93e3-775f0d349644",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generated_notes = []\n",
    "for i in tqdm(range(n), desc=\"Generating notes\", total=n):\n",
    "    prompt = make_prompt(PROMPT_TMPL, SCENARIOS)\n",
    "    temp      = random.uniform(0.7, 1.0)\n",
    "    top_p     = random.uniform(0.7, 1.0)\n",
    "    max_tokens  = random.randint(700, 1800)\n",
    "    seed      = random.randint(0, 2**30)\n",
    "\n",
    "    raw = generate_from_vllm(\n",
    "        prompt=prompt,\n",
    "        temperature=temp,\n",
    "        top_p=top_p,\n",
    "        max_tokens=max_tokens,\n",
    "        seed=seed,\n",
    "        retries=3,\n",
    "        delay=1.0\n",
    "    )\n",
    "    \n",
    "    note = remove_end_tag(raw, end_token=\"***END NOTE***\")\n",
    "    generated_notes.append(note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114855aa-cc8b-473b-8718-d3d3b54c69f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "outputs = []\n",
    "for i in tqdm(range(n), total=n, desc=\"Generating notes \"):\n",
    "    note = generate_from_vllm(base_prompt)\n",
    "    outputs.append({\"id\": i, \"note-excerpt\": remove_end_tag(note)})\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a4545a-c553-4a7a-8a0d-6cce89db0d4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sampled_outputs = random.sample(generated_notes, min(n, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2725ef1-e148-4e66-b42c-7b114caeb096",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for sample in sampled_outputs:\n",
    "    print((sample))\n",
    "    print(len(sample))\n",
    "    print(\"\\n\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45b1c6c-b686-4179-bbd1-4fd8c2d2cf25",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98894e0-a761-484a-ad78-c33ec56f300f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "out_path = OUTPUT_DIR / f\"generated_{prompt_path.stem}_{timestamp}.csv\"\n",
    "pd.DataFrame(generated_notes).to_csv(out_path, index=False)\n",
    "print(f\"✅ Saved: {out_path}\")\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0515c68e-77da-4d74-8ebd-5fcd815522b2",
   "metadata": {},
   "source": [
    "# Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f305e1f-3105-4bb9-a15f-a8bde487c2d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_from_vllm_old(prompt: str, temperature=0.7, max_tokens=1024, retries=3, delay=2, top_p = 0.95) -> str:\n",
    "    \"\"\"Send prompt to local vLLM server and return the generated text.\"\"\"\n",
    "    payload = {\n",
    "        \"prompt\": prompt,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"top_p\": top_p\n",
    "    }\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.post(VLLM_URL, json=payload)\n",
    "            response.raise_for_status()\n",
    "            return response.json()[\"choices\"][0][\"text\"].strip()\n",
    "        except Exception as e:\n",
    "            print(f\"[Warning] Generation failed (attempt {attempt+1}/{retries}): {e}\")\n",
    "            time.sleep(delay)\n",
    "    return \"[Generation Failed]\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
